
### Challenge 1: What is the difference between expected value and mean?


Expected value is the average value of a random variable over a large number of experiments. 

A random variable maps numeric values to each possible outcome in an experiment.



### Challenge 2: What is the "problem" in science with p-values?


The American Statistical Association defines the p-value as, informally: the probability under a specified statistical model that a statistical summary of the data would be equal to or more extreme than its observed value.

Misuse of p-values is common in scientific research and scientific education. p-values are often used or interpreted incorrectly: 

- Framework Problems: There are several problems with the null hypothesis significance testing framework.
   
   - Null hypotheses are made to be rejected. 
   
   - How you collect the data matters. Another, oft-unstated assumption about the p-value calculation centres on data collection. If we break this assumption, the p-value only matters in the alternate universe where we did not.
   
   - Distributional assumptions matter. The underlying distribution we given to our data is important. This is part of the assumptions that go into a p-value calculation. There can be major differences in p-values using the same data under different models

    - Classical statistics does not assign probabilities to hypotheses.: The classical definition of a probability is the long-run frequency of an event. This definition means we cannot give hypotheses a probability. That hypothesis holds or not.


### Challenge 3: Applying testing to a specific case: A/B testing.
A/B testing is a widely used tool to understand differences between two samples. It is a way to measure the impact of something we did: 
* A marketing campaign.
* A new feature in our application. 
* A new design in our application.
* A different flow in the User Experience flow.

To do this, is very important first to design our experiment. 
* We need to know how we are measuring the impact. If people has the behaviour we want with this new implementation.
* We choose a control group (people who doesn't have/see the new change) and the group which will see the new change. 
* We think about how much data do we need.
* We measure the difference between them.

One example:
Our application has a lot of mini-games. We want people to reach the games that we think are the best but the behaviour is not the expected, they don't reach them.

So we call a designer and after a lot of work he shows us a new design for our application: we will add a button specific for that kinf of games inviting the users to click on it:

*Click here to discover cool games!*

We think it will work but can we be sure? So instead of implementing this new botton for all users, we implement it for 10% and we compare the results with the users that didn't have it. Is there a significant difference? Is our botton working?

Read more about A/B testing with a couple of examples:

[Another example about Netflix here](http://select.video/artwork4)

[What happened to Basecamp](http://millions.social/tested7)

[An example with Python](http://math.social/tested3)

[A cool general explanation](http://arts.show/tested7)

So, take one single example in the articles you just read, which specific test/s would you apply? (We want you just to do a draft and think a little bit how to apply the tests you already know in this case)

For the BaseCamp example: 

This is a two sample hypotesis test, both sample are independant, I would thus use

from scipy.stats import ttest_ind

# INDEPENDENT samples: we look at the difference of the means
# mu(after) - mu(before)
# H0: there is no difference
# H1: there is a difference

# equal variance
ttest_ind(ab_test.a, ab_test.brel, equal_var=True)





